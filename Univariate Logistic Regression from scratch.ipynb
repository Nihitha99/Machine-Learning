{"cells":[{"metadata":{"_uuid":"c3d560a5aed749f62d815529bf43ec50dee7ad21"},"cell_type":"markdown","source":"# Univariate Logistic Regression"},{"metadata":{"_uuid":"067ae8f7371e5c298016f774cc43314f673f18fc"},"cell_type":"markdown","source":"Logistic regression is a supervised classification algorithm \nThis is similar to linear regression, but is used when the dependent variable is categorical (despite its name regression). So, it targets the classification problems unlike linear regression for regression problems. Objective in classification is to map the observation (input) to its associated class or label. \n\n\n\nLinear regression can be used for classification problems by using a threshold, however using least squares (the sum of the squares of the residuals) criteria is not more apt way for separating the classes for classification. \n\nSo, usually for categorical values best try is Logistic Regression, Decision Trees, SVM and Random Forest etc. \n\nLogistic regression predicts the probability of the outcome for a given input unlike linear function which predicts the outcome for a given input. <br>\nSo, output value modeled in logistic regression is a binary value (0 or 1) instead of a numeric value in linear regression. \n"},{"metadata":{"_uuid":"97bf38d452bf1445b04cf829b5d4203483de5ba9"},"cell_type":"markdown","source":"Say P is the probability of success of an event. Then probability of failure is 1-P. <br>\nBy definition, probabilities and proportions cannot exceed 1 or fall below 0. \nOdds of success are defined as ratio of probability of success over probability of failure. <br>\nOdds = P/(1-P)"},{"metadata":{"_uuid":"d238b0f819919a02bccc6050156adeb48d9eaf8b"},"cell_type":"markdown","source":"$$ log(\\dfrac{P(x)}{1-P(x)}) = \\sum_{i=1}^{m} \\theta_0 + \\theta_i x_i $$\nIn vector form $$ =\\theta^T X $$"},{"metadata":{"_uuid":"bb5aad7a7e26988157d17b874964f687bec44146"},"cell_type":"markdown","source":"This model assumes that the log(odds) of an observation can be expressed as a linear function to the input variable. "},{"metadata":{"_uuid":"bc14cc498619f89a686d499da3ab7f96d4938aaf"},"cell_type":"markdown","source":"LHS is logit of P, so the name logistic regression. <br>\nRHS is linear, similar to linear regression. <br>\n\nInverse of the logit function is \n"},{"metadata":{"_uuid":"feabeeb95b5f9a5fafe693e8b6d65c23fb61dcc6"},"cell_type":"markdown","source":"$$P(x) =  \\dfrac{e^z}{1+e^z} $$\n\n\n $$ = \\dfrac{1}{1+e^{-z}}$$\nwhere $z = \\theta^T X$"},{"metadata":{"_uuid":"6541e7a752d35414f6d8d3a5a1293fed12aa8c67"},"cell_type":"markdown","source":"RHS is the sigmoid function of Z, which maps the continuous real line to the interval (0,1)\n"},{"metadata":{"_uuid":"5b79a1dbb2f3c31c355d30e141931937f57fefbc"},"cell_type":"markdown","source":"### Flow chart"},{"metadata":{"_uuid":"2f215ac7f9b2f319efc3cd74df4c68f8671423d5"},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1gVpO1m_o-f-IAl-udWMGZu7C6QENAMNS)"},{"metadata":{"_uuid":"a155df8de94b5ed7f697122cbca125614c7b0664"},"cell_type":"markdown","source":"Algorithm for this is very similar to [linear regression](https://www.kaggle.com/rakend/simple-linear-regression-using-gradient-descent), only things we need to change is the representation of response(prediction) and the cost function\n\nThe response, while in linear regression we use a linear function of X ($\\theta^T X$), in logistic regression we use sigmoid function of the linear function ($\\theta^T X$). <br>\n"},{"metadata":{"_uuid":"91c66d6f80e483fd58894e8e4e15e5fccdf0dd35"},"cell_type":"markdown","source":"Hypothesis function: <br>\n Linear Regression :  $$h_\\theta(x)  = (\\theta^T X)$$  \n \n Logistic Regression : \n $$h_\\theta(x) =  g(\\theta^T X)$$"},{"metadata":{"_uuid":"bd4723c9cb2b13ee11d2f271b7e14bb659c0cc43"},"cell_type":"markdown","source":"$g$ is a link function, this transforms the observed responses to the original data. \n"},{"metadata":{"_uuid":"e9d533edbfafdee98cb872b55a22025483e8cff7"},"cell_type":"markdown","source":"$$h_\\theta(x) = \\dfrac{1}{1+e^{-\\theta^T X}}$$"},{"metadata":{"_uuid":"cf722111608937f19c63be6109f7c6e24cce3e59"},"cell_type":"markdown","source":"This hypothesis function represents the estimated probability that y = 1 for given input x parameterized by Î¸:\n$$h_\\theta(x) = P(y=1 | x;\\theta)$$"},{"metadata":{"_uuid":"0efae9bf31caf9e470a67b5ab7a7e5045c30c051"},"cell_type":"markdown","source":"Since the hypothesis function is formed by sigmoid function, our cost function is not going to be a convex function, which happened to be for linear regression as our hypothesis function  was a linear function. \nIt means, unlike the cost function in linear regression, this cost function in logistic regression will get many local minimum.  <br>\nSo, **to make this a convex function, this is transformed using logarithm**."},{"metadata":{"_uuid":"ecd499535bff2e11bfd0c7da5e2c96415396870d"},"cell_type":"markdown","source":"![]( https://i.stack.imgur.com/ufmSH.png)"},{"metadata":{"_uuid":"5b1214f0602f0c8eb38e1b71e33c32bca5847f01"},"cell_type":"markdown","source":"Image source:Coursera, Neural Networks"},{"metadata":{"_uuid":"29644ea499d9dc54bc15c42269869ee71bc6844b"},"cell_type":"markdown","source":"$$  Error(h_\\theta(x),y)=\\begin{cases}\n    -log(h_\\theta(x)), & \\text{if $y=1$}\\\\\n     -log(1-h_\\theta(x)), & \\text{if $y=0$}\n  \\end{cases}$$"},{"metadata":{"_uuid":"1c6b36648c0c7aca037a1367c463b60f0a0429a7"},"cell_type":"markdown","source":"Cost function : <br>\n\nMean of the errors \n$$J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^{m}Error(h_\\theta(x^{(i)})),y^{(i)})$$"},{"metadata":{"_uuid":"a4253c07f08b2bf7ede2858775a2cf5b4da16923"},"cell_type":"markdown","source":"We are trying to minimize the difference between the prediciont and the response"},{"metadata":{"_uuid":"2943a5c1458b7a070779165a16d2e58afd833075"},"cell_type":"markdown","source":"Putting the two functions into one compact function, we get\n$$J(\\theta) = -\\dfrac{1}{m} \\sum_{i=1}^{m} \\big[y^{(i)} log(h_\\theta(x^{(i)})) +(1-y^{(i)}) log(1-h_\\theta(x^{(i)}))\\big] $$"},{"metadata":{"_uuid":"b3ecb9b6585224b4055612c8fc785cdbcd999611"},"cell_type":"markdown","source":"Binomial/Binary/Univariable/Univariate logistic regression - If the dependent variable is a binary variable [True/False, 0/1]\n\nMultinomial logistic regression - If the dependent variable have more than two outcomes. \n\n"},{"metadata":{"_uuid":"adb6040dd3876a6a40c2a0fcd99cf731bbbd8f6e"},"cell_type":"markdown","source":"We'll try for univariate logistic regression. <br>\nWe use iris data from sklearn datasets for this. <br>\nFor that we will consider only one feature (sepal length) and a target vector of size 2 (setosa, versicolor)"},{"metadata":{"_uuid":"c8bd938e5bf013ad7b3043c4a6b30d3a0da0d45e"},"cell_type":"markdown","source":"### Working"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2c8d99e6b0ad55e6c97bb9b1430887767cd44956"},"cell_type":"code","source":"\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn import linear_model","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"23976813b517707c170e9701b294b809c022f357"},"cell_type":"code","source":"iris = datasets.load_iris()","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"98d04200f0c639246fa007c38e60a58161975431","collapsed":true},"cell_type":"code","source":"iris.feature_names","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a30a4bd417f70301a7e245ca80cde68ec25f6271","collapsed":true},"cell_type":"code","source":"iris.target_names","execution_count":4,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0ec5f640bba95a427ebae93bd4cc404bc65a7c98"},"cell_type":"code","source":"X = iris.data[:, 0]\n\ny_bool = iris.target!=2\n\ny = iris.target[y_bool]\n\nX = X[y_bool]","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f3d8e5e5e16db2d45ac630b9349d3af20ef759eb","collapsed":true},"cell_type":"code","source":"plt.scatter(X, y)\nplt.xlabel('Sepal Length ', fontsize=15)\n\nplt.ylabel('0 - setosa, 1 - versicolor ', fontsize=15)\nplt.show()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"5365ed57df5e18d570b988174626b7eb7fa66b10"},"cell_type":"markdown","source":"#### Algorithm"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"582acdbdac16e29192c6c2c4f3e578802af7d6b4"},"cell_type":"code","source":"X = np.c_[np.ones((X.shape[0],1)), X[:]]\ny = y.reshape(-1,1)\n\n\n# Parameters required for Gradient Descent\nalpha = 0.1   #learning rate\nm = y.size  #no. of samples\nnp.random.seed(10)\ntheta = np.random.rand(2)  #initializing theta with some random values\ntheta = theta.reshape(-1,1)","execution_count":7,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"171d7f4ce051107e26ca356e7e2d2b7b70f2fc04"},"cell_type":"code","source":"def gradient_descent(x, y, m, theta,  alpha):\n    cost_list = []   #to record all cost values to this list\n    theta_list = []  #to record all theta_0 and theta_1 values to this list \n    prediction_list = []\n    run = True\n    cost_list.append(1e10)    #we append some large value to the cost list\n    i=0\n    while run:\n        Z = np.dot(x, theta) \n        prediction = 1 / (1 + np.exp(-Z))   #predicted y values \n        prediction_list.append(prediction)\n        error = prediction - y\n        cost = np.sum(-(y * np.log(prediction) + (1 - y) * np.log(1 - prediction))) / m   #  (1/2m)*sum[(error)^2]\n        \n        cost_list.append(cost)\n        theta = theta - (alpha * (1/m) * np.dot(x.T, error))   # alpha * (1/m) * sum[error*x]\n        theta_list.append(theta)\n        if cost_list[i]-cost_list[i+1] < 1e-9:   #checking if the change in cost function is less than 10^(-9)\n            run = False\n\n        i+=1\n    cost_list.pop(0)   # Remove the large number we added in the begining \n    return prediction_list, cost_list, theta_list","execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2918a03f78918d573173dc7941399058c8a03a55"},"cell_type":"code","source":"prediction_list, cost_list, theta_list = gradient_descent(X, y, m, theta, alpha)\ntheta = theta_list[-1]","execution_count":9,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6de6b18df1ea57842034b632f43c745b88d5d5d2","collapsed":true},"cell_type":"code","source":"plt.title('Cost Function J', size = 30)\nplt.xlabel('No. of iterations', size=20)\nplt.ylabel('Cost', size=20)\nplt.plot(cost_list)\nplt.show()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"33c84ecbda4d4cf4c9d3a592f9990b436d4e8b3e"},"cell_type":"markdown","source":"Weights we got"},{"metadata":{"trusted":false,"_uuid":"6f682ccbd015fc5fa44c4285f9ff9d1395678586","collapsed":true},"cell_type":"code","source":"theta","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"7c25cb8b9251bd33ddaf1d3f8e64566e84a66e9d"},"cell_type":"markdown","source":"We'll plot a decision boundary with the coefficients"},{"metadata":{"_uuid":"e355882d805ced5520c4b7b847884960a783c426"},"cell_type":"markdown","source":"We'll take sample data in the training data range"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"35f391b0deaf30bb580bfebba9ef912827a8f87b"},"cell_type":"code","source":"X_test = np.linspace(4, 7, 300)","execution_count":12,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2a9afd2d17e497768075a0a7899b8d8296f81dc9"},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))","execution_count":13,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d9f799cf2121dc43f44d2e045279de73bd9aa4b5"},"cell_type":"code","source":"loss = sigmoid(X_test*theta[1] + theta[0])","execution_count":15,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3a15a35230225364dc6a98c7b03adb2a56113c73","collapsed":true},"cell_type":"code","source":"plt.figure(1, figsize=(8, 6))\nplt.clf()\nplt.plot(X_test, loss, c='C0', label='Hyperplane')\nplt.scatter(X[:,1], y, c='C1', label='Training data')\n# plt.plot(X_test,X_test*theta[1] + theta[0] )\n# plt.axhline(0.5, c='C2',label='0.5 Threshold')\n# plt.axvline(5.4147157190635449, c='C2',label='0.5 Threshold')\n# plt.axvline\nplt.legend()\nplt.xlabel('Sepal Length (Input)', fontsize=15)\nplt.ylabel(\"Probability of the output\" \"\\n\" \"(0 - setosa, 1 - versicolor)\", fontsize=15)\nplt.show()","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"a02d76f2c946afc6739e49fbc1efcd7754400449"},"cell_type":"markdown","source":"We keep this threshold value of 0.5, for the given input if the probability is equal to or above 0.5, we consider it 1 and less than 0.5 we consider it 0"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"9f9b277c2d20b67c99681360566ac306e3497da3","collapsed":true},"cell_type":"code","source":"# 0.5 threshold corresponds to \nboundary = X_test[np.where(loss >= 0.5)[0][0]]\nprint(round(boundary,3))","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"67320c477ee5bdb0aea54475bf03ee6cf40044c3"},"cell_type":"markdown","source":"So, sepal length above 5.415 is categorized as versicolor and less than that is categorized as setosa"},{"metadata":{"trusted":false,"_uuid":"8792b2e49d650e11ad604f89a54dbacd760bf768","collapsed":true},"cell_type":"code","source":"plt.figure(1, figsize=(8, 6))\nplt.clf()\nplt.plot(X_test, loss, c='C0', label='Hyperplane')\nplt.scatter(X[:,1], y, c='C1', label='Training data')\nplt.axhline(0.5, c='C2',label='0.5 Threshold', linewidth=0.7)\nplt.axvline(boundary, c='C3',label='Boundary', linewidth=0.7)\nplt.legend()\nplt.xlabel('Sepal Length (Input)', fontsize=15)\nplt.ylabel(\"Probability of the output\" \"\\n\" \"(0 - setosa, 1 - versicolor)\", fontsize=15)\nplt.show()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"e11f021bb88859cbcc2bcac3a2509dc16b2fcee2"},"cell_type":"markdown","source":"## Scikit learn"},{"metadata":{"_uuid":"474ea9ff86bc7998a2c7f257f711f3607cbdbc31"},"cell_type":"markdown","source":"We can check this with sci-kit learn logistic regression module"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a6a05861a4aa7f81e8b72adb40729537526d91ff"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7d67b5ab1bf7f6428f2d0b8afb2564e2040769fd"},"cell_type":"code","source":"lr = LogisticRegression(C=1e100)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"a6f27f273558614d881f8c15025d9e8455d5f72b"},"cell_type":"markdown","source":"Here C from the formal definition from sklearn <br>\nInverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n\n"},{"metadata":{"_uuid":"65fceb94503d2d14c6ad29a2673679e28675e55b"},"cell_type":"markdown","source":"Since we are not using any regularization here, we need to keep C ($=1/\\lambda$) value very high, so the the regularization strength $\\lambda \\approx 0$"},{"metadata":{"_uuid":"7690b53d3b2bc508e59ada952ec2ee6498881060"},"cell_type":"markdown","source":"Regularization is used for avoiding overfitting. <br>\nWe will see more about regularization in another post. "},{"metadata":{"trusted":false,"_uuid":"37043a150613ceca5fa09c890c33eb1cb3b07690","collapsed":true},"cell_type":"code","source":"X.shape","execution_count":21,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b6451cc6f3982a45e8e26c268801939d0363046c","collapsed":true},"cell_type":"code","source":"X[:5]","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"39cb064b7ab643ebeab66106f5bc3309e148132f"},"cell_type":"markdown","source":"We added a column of ones for our earlier algorithm, which we don't need here, so taking off that column"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8f79903c7762152e68a3854dad04ed4c1dcbcc68"},"cell_type":"code","source":"lr = lr.fit(X[:,1].reshape(-1,1),y.ravel())","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"1516089aff71758fe65a2e2a4e56ab2670a3d4c1"},"cell_type":"markdown","source":"Checking the weights"},{"metadata":{"_uuid":"56a9b5f331b3bbdccf21569680098530536895cd"},"cell_type":"markdown","source":"#### From scikit learn"},{"metadata":{"trusted":false,"_uuid":"48cfd04b3c1c24853c275909c67f9a2115e7aa64","collapsed":true},"cell_type":"code","source":"'Theta_0 and Theta_1 are {},{}'.format(round(lr.intercept_[0],3), round(lr.coef_[0,0],3))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"b3586ce5951847d63c9289a6b3fe5d92d79af420"},"cell_type":"markdown","source":"#### From scratch"},{"metadata":{"trusted":false,"_uuid":"27be43a38f1ac7c58845e6e691606784554ba5a9","collapsed":true},"cell_type":"code","source":"theta[0,0], theta[1,0]","execution_count":30,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"974185b5a5662a8590aa72e0c508c52106bb5d45","collapsed":true},"cell_type":"code","source":"'Theta_0 and Theta_1 are {},{}'.format(round(theta[0,0],3),round(theta[1,0],3))","execution_count":31,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}